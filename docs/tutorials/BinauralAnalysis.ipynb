{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b060e6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Using Soundscapy for Binaural Recording Analysis\n",
    "\n",
    "Soundscapy has evolved to provide a comprehensive suite of acoustic and psychoacoustic analyses. This tutorial will guide you through using the new `AcousticAnalysis` class, which serves as the primary interface for performing these analyses. The system is optimized for batch processing, ease of use, and reproducibility.\n",
    "\n",
    "## Background\n",
    "\n",
    "Soundscapy relies on three main packages to provide its analysis functions:\n",
    "\n",
    "1. [Python Acoustics](https://github.com/python-acoustics/python-acoustics) (`acoustics`): Provides standard acoustic metrics with direct references to relevant standards.\n",
    "2. [scikit-maad](https://scikit-maad.github.io) (`maad`): Offers a suite of ecological soundscape and bioacoustic indices.\n",
    "3. [MoSQITo](https://github.com/Eomys/MoSQITo) (`mosqito`): Provides key psychoacoustic metrics.\n",
    "\n",
    "The metrics available include:\n",
    "- From Python Acoustics: $L_{Zeq}$, $L_{Aeq}$, $L_{Ceq}$, SEL, and associated statistics.\n",
    "- From scikit-maad: Temporal and spectral alpha indices.\n",
    "- From MoSQITo: Loudness, Sharpness, and Roughness.\n",
    "\n",
    "Soundscapy combines all of these metrics and makes it easy and (relatively) fast to compute any or all of them for a binaural audio recording. These results have been preliminarily confirmed through comparison of results obtained from Head Acoustics ArtemiS suite on a set of real-world recordings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91ed01",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's begin by importing the necessary modules and setting up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf0f9fb",
   "metadata": {
    "lines_to_next_cell": 2,
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:31.913447Z",
     "start_time": "2023-08-12T20:19:30.930268Z"
    }
   },
   "source": [
    "# imports\n",
    "from soundscapy import AudioAnalysis\n",
    "from soundscapy import AnalysisSettings\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "68fd3ee4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": "Set up where the data is located. In this case, we'll use the sample recordings located under the `test` folder."
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f299664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:31.919300Z",
     "start_time": "2023-08-12T20:19:31.914511Z"
    }
   },
   "source": [
    "# May need to adjust for your system\n",
    "wav_folder = Path().cwd().parent.parent.joinpath(\"test\", \"data\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "556fc578",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Calibration Levels\n",
    "\n",
    "Ensuring correct calibration is crucial for accurate analysis. If you used equipment such as the Head Acoustics SqoBold, and were careful about how the recordings are exported to .wav, then they may already be correctly adjusted (as ours are here). However its best to be safe and calibrate each signal to their real-world dB level. To do this, we load in a .json that contains the per-channel correct dB $L_{eq}$ level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f78f14c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:31.920661Z",
     "start_time": "2023-08-12T20:19:31.916705Z"
    }
   },
   "source": [
    "levels = wav_folder.joinpath(\"Levels.json\")\n",
    "\n",
    "with open(levels) as f:\n",
    "    levels = json.load(f)\n",
    "\n",
    "# Look at the first five sets of levels\n",
    "list(levels.items())[:5]"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initializing AudioAnalysis\n",
    "\n",
    "The `AudioAnalysis` class is our main interface for performing acoustic analyses. Let's initialize it with default settings:\n"
   ],
   "id": "e059d75091837b35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T12:55:43.697194Z",
     "start_time": "2024-08-20T12:55:43.610501Z"
    }
   },
   "cell_type": "code",
   "source": "analysis = AudioAnalysis()",
   "id": "2bbd2c09e608b0b5",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AudioAnalysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m analysis \u001B[38;5;241m=\u001B[39m \u001B[43mAudioAnalysis\u001B[49m()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'AudioAnalysis' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "52b6337863b0627f"
  },
  {
   "cell_type": "markdown",
   "id": "34481ae7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "### Prepping the results dataframe\n",
    "The easiest way to organise and add the new data as it is processed is to prepare a dataframe ahead of time. We've provided a small function to convert a dictionary of calibration levels (`level`) into the properly formatted dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb2fc0c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:31.981839Z",
     "start_time": "2023-08-12T20:19:31.968684Z"
    }
   },
   "source": [
    "df = prep_multiindex_df(levels, incl_metric=True)\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8e138497",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "#### Load in a Binaural recording\n",
    "Load in a binaural wav signal. We can use the `plot` function provided by the `acoustics.Signal` super-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76bc2e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:34.148763Z",
     "start_time": "2023-08-12T20:19:32.948873Z"
    }
   },
   "source": [
    "binaural_wav = wav_folder.joinpath(\"CT101.wav\")\n",
    "b = Binaural.from_wav(binaural_wav)\n",
    "b.plot();"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a17c3655",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "To ensure that the dB level is correct, and therefore any other metrics are correct, we start by calibrating the signal to precalculated levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68dc1464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:34.214107Z",
     "start_time": "2023-08-12T20:19:34.149622Z"
    }
   },
   "source": [
    "decibel = (levels[b.recording][\"Left\"], levels[b.recording][\"Right\"])\n",
    "print(f\"Calibration levels: {decibel}\")\n",
    "b.calibrate_to(decibel, inplace=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "87001293",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now, check it by comparing it to what we already knew were the correct levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b3cedc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:34.841514Z",
     "start_time": "2023-08-12T20:19:34.826178Z"
    }
   },
   "source": [
    "print(f\"Predefined levels: {levels[b.recording]}\")\n",
    "print(f\"Calculated Levels: {b.pyacoustics_metric('Leq', statistics=['avg'], as_df=False)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "407924ea",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Calculating Acoustic Metrics\n",
    "\n",
    "This brings us to how to calculate any of the many metrics available. Let's start simple with $L_{Aeq}$.\n",
    "\n",
    "### Python Acoustics\n",
    "\n",
    "Since the $L_{Aeq}$ calc is provided by the Python Acoustics library, we'll be calling `pyacoustic_metric`. Then, we need to tell it what particular metric we want, what stats to calculate as well, what to label it, and what format to return the results in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35968c0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:35.772475Z",
     "start_time": "2023-08-12T20:19:35.688843Z"
    }
   },
   "source": [
    "metric = \"LAeq\"\n",
    "stats = (\"avg\", 10, 50, 90, 95, \"max\")\n",
    "label = \"LAeq\"\n",
    "b.pyacoustics_metric(metric, stats, label, as_df=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c2969062",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "If we want, we can get the results back as a pandas DataFrame instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1bb36e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:36.991090Z",
     "start_time": "2023-08-12T20:19:36.920368Z"
    }
   },
   "source": [
    "b.pyacoustics_metric(metric, stats, label, as_df=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "be992443",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "And we can easily do the same for the C-weighting level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55da2d6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:38.137315Z",
     "start_time": "2023-08-12T20:19:38.055350Z"
    }
   },
   "source": [
    "b.pyacoustics_metric(\"LCeq\", stats, as_df=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f5786a3c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### MoSQITo\n",
    "\n",
    "MoSQITo is very exciting as it is one of the first completely free and open-source libraries for calculating psychoacoustic features. Let's try out calculating the psychoacoustic loudness.\n",
    "\n",
    "We start by defining many of the same options, but with two new ones. The first is our `func_args` to pass to `MoSQITo`. Since our test recording was collected in a public park, we need to select the correct field type: free or diffuse, and pass that to MoSQITo.\n",
    "\n",
    "The second new argument is `parallel`. This just tells Soundscapy whether to try to calculate the Left and Right channels simultaneously to speed up processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce383d8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:09.848885Z",
     "start_time": "2023-08-12T20:19:39.106261Z"
    }
   },
   "source": [
    "metric = \"loudness_zwtv\"\n",
    "stats = (5, 50, 'avg', 'max')\n",
    "func_args = {\n",
    "    'field_type': 'free'\n",
    "}\n",
    "\n",
    "b.mosqito_metric(metric, statistics=stats, as_df=True, parallel=True, verbose=True, func_args=func_args)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fd19d5f1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "`sharpness_din_from_loudness` is a bit of a special case to keep in mind. It can drastically speed up the processing time since it calculates the Sharpness values from pre-calculated Loudness results. If you are planning to do both analyses, I highly suggest using it. Soundscapy will handle it behind the scenes to make sure it doesn't accidentally calculate the Loudness values twice if you've asked for both of them. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6edbbf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:40.423258Z",
     "start_time": "2023-08-12T20:20:09.850003Z"
    }
   },
   "source": [
    "b.mosqito_metric(\"sharpness_din_from_loudness\", stats, as_df=True, parallel=True, verbose=True, func_args=func_args)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "431afc2f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "By default, the metrics will be calculated for both channels. But you may want only a single channel. This can be set with the `channel` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3342978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:40.450543Z",
     "start_time": "2023-08-12T20:20:40.424170Z"
    }
   },
   "source": [
    "b.pyacoustics_metric(\"LZeq\", channel=\"Left\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5e580b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:41.243999Z",
     "start_time": "2023-08-12T20:20:40.444995Z"
    }
   },
   "source": [
    "b.maad_metric(\"all_spectral_alpha_indices\", verbose=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "b.maad_metric(\"all_temporal_alpha_indices\", verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T20:59:10.379510Z",
     "start_time": "2023-08-12T20:59:10.296558Z"
    }
   },
   "id": "ab9bbfd3d017d317",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7ee05cbb",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Defining Analysis Settings\n",
    "\n",
    "Soundscapy provides the ability to predefine your analysis settings. These are defined in a separate `.yaml` file and are managed by Soundscapy using the `AnalysisSettings` class. These settings can then be passed to any of the analysis functions, rather than separately defining your settings as we did above. This will be particularly useful when performing our batch processing on an entire folder of wav recordings later.\n",
    "\n",
    "Soundscapy provides a set of default settings which can be easily loaded in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43f4791c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:41.251435Z",
     "start_time": "2023-08-12T20:20:41.244569Z"
    }
   },
   "source": [
    "analysis_settings = AnalysisSettings.default()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "01cdf86b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "However, in your own analysis you'll probably want to define your own options and load that in. We'll show how this is done using the `example_settings.yaml' file. First, let's take a look at how it's laid out:\n",
    "\n",
    "```\n",
    "# Settings file for batch acoustic analysis.\n",
    "# Split up according to which library performs which analysis.\n",
    "\n",
    "# Python Acoustics\n",
    "# Supported metrics: LAeq, LZeq, LCeq, SEL\n",
    "# Supported stats: avg/mean, max, min, kurt, skew, any integer from 1-99\n",
    "PythonAcoustics:\n",
    "    LAeq:\n",
    "        run: true\n",
    "        main: 'avg'\n",
    "        statistics: [5, 10, 50, 90, 95, 'min', 'max', 'kurt', 'skew']\n",
    "        channel: [\"Left\", \"Right\"]\n",
    "        label: 'LAeq'\n",
    "        func_args:\n",
    "            time: 0.125\n",
    "            method: \"average\"\n",
    "\n",
    "    LZeq:\n",
    "        run: true\n",
    "        main: 'avg'\n",
    "        statistics: [5, 10, 50, 90, 95, 'min', 'max', 'kurt', 'skew']\n",
    "        channel: [\"Left\", \"Right\"]\n",
    "        label: 'LZeq'\n",
    "        func_args:\n",
    "            time: 0.125\n",
    "            method: \"average\"\n",
    "\n",
    "# MoSQITo\n",
    "# supported metrics: loudness_zwtv, sharpness_din_from_loudness, roughness_dw\n",
    "# supported stats: avg/mean, max, min, kurt, skew, any integer from 1-99\n",
    "MoSQITo:\n",
    "    loudness_zwtv:\n",
    "        run: true\n",
    "        main: 5\n",
    "        statistics: [10, 50, 90, 95, 'min', 'max', 'kurt', 'skew', 'avg']\n",
    "        channel: [\"Left\", \"Right\"]\n",
    "        label: \"N\"\n",
    "        parallel: true\n",
    "        func_args:\n",
    "            field_type: \"free\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d69b62",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "The settings file is broken up according to the three libraries. Within these, we define separate options for each metric to calculate. The name of this metric should correspond exactly with what Soundscapy expects (in the case of PythonAcoustics) or what the underlying library calls its function.\n",
    "\n",
    "Within each function, we then have a collection of settings that Soundscapy uses:\n",
    "    * `run` : This tells Soundscapy whether or not to actually run this metric. This allows you to define and save the options you use for each metric without needing to run it.\n",
    "    * `main` and `statistics` : These are the sub-statistics to calculate (e.g. $L_{5}$, $L_{90}$, etc.). `main` operates just like any of these, except you also have the option to return only the main statistic to simplify your results while still leaving your other preferences intact.\n",
    "    * `label` : What label to assign that metric. For instance, Loudness is typically `'N'`. When calculated, the statistics will be appended like so: N_5, N_10, N_avg, ... N_{`stat`} and this will be the column name for that metric. If you pass nothing here, then Soundscapy will fall back to the labels defined in `sq_metrics.DEFAULT_LABELS`. **Warning**: Some functions share a label (e.g. `sharpness_din_tv` and `sharpness_din_perseg` are both 'S'), if you run both of these and don't define different labels, one will overwrite the other.\n",
    "\n",
    "Finally, there is an opportunity to define arguments to pass to the underlying function itself. This is perhaps the most important part for consistency and reproducibility. This is where you define which standard is being used, what time or frequency weighting, or what spectrogram bins to use. These options are defined by the 3 analysis libraries used and are not documented fully in Soundscapy. When the `AnalysisSettings` is parsed, `func_args` will be returned as a `dict` with an entry for each option you'd like to pass. `func_args` is then passed as `**kwargs` to e.g. the `mosqito.sq_metrics.loudness_zwtv()` function. If `func_args` contains an option the function doesn't recognise it will throw an error, so be careful when defining these arguments.\n",
    "\n",
    "---------------------------------------\n",
    "Let's try loading in the `example_settings.yaml` file and see how `AnalysisSettings` handles it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "424399f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:44.485117Z",
     "start_time": "2023-08-12T20:20:44.481932Z"
    }
   },
   "source": [
    "ex_settings = AnalysisSettings.from_yaml(Path(\"example_settings.yaml\"))\n",
    "ex_settings['scikit-maad']['all_temporal_alpha_indices']['run'] = True\n",
    "ex_settings['scikit-maad']['all_spectral_alpha_indices']['run'] = True\n",
    "ex_settings"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4ddfdb81",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "`ex_settings` is just a Python `dict` with some class methods added on. One of these is a function to parse the settings object for a specific library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6c6f7fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:45.718584Z",
     "start_time": "2023-08-12T20:20:45.705121Z"
    }
   },
   "source": [
    "ex_settings.parse_pyacoustics(metric=\"LAeq\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e822f46b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "This returns the value for `run`, `channel`, `statistics`, `label`, and `func_args` which will then be used by the `pyacoustic_metric()` function to calculate the $L_{Aeq}$ and its stats.\n",
    "\n",
    "When passing your settings to an analysis function, it will start by automatically parsing and applying the settings for that particular metric. This will override any other settings passed to the function, so if you're using a settings file and you want to change anything, you should either change it in the `.yaml` and reload the settings. This also makes sure you keep a record of the settings for the last time you ran the analysis.\n",
    "\n",
    "You can easily reload the settings `.yaml` after changing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5da5ee5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:47.717749Z",
     "start_time": "2023-08-12T20:20:47.704543Z"
    }
   },
   "source": [
    "# ex_settings = ex_settings.reload()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "89382227",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Running a single metric with predefined settings\n",
    "\n",
    "Now, with our settings loaded, we process a recording using those settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2700bdee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:50.182201Z",
     "start_time": "2023-08-12T20:20:50.095370Z"
    }
   },
   "source": [
    "b.pyacoustics_metric(\"LAeq\", analysis_settings=ex_settings)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0314af2c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "But this is just the start of what makes the analysis settings so useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a584619",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Processing all the metrics at once, using predefined analysis settings.\n",
    "\n",
    "Since we can define the settings for all the metrics, and specify which metrics we want to run, we can process all of our desired metrics at once. `process_all_metrics()` let's us do this with just a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e6ebf83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:53.694900Z",
     "start_time": "2023-08-12T20:20:52.711936Z"
    }
   },
   "source": [
    "b.process_all_metrics(ex_settings, verbose=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "61fde748",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "In this case we left out the MoSQITo metrics since they take a while, but let's say you do want to run those. You could either edit the `example_settings.yaml` file and use `ex_settings.reload()`, or what we'll do within this notebook is just directly edit the underlying dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dc4cbd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:21:27.391295Z",
     "start_time": "2023-08-12T20:20:55.665253Z"
    }
   },
   "source": [
    "ex_settings['MoSQITo'][\"loudness_zwtv\"][\"run\"] = True\n",
    "\n",
    "b.process_all_metrics(ex_settings, verbose=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1d20f438",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "And now we have all of the same metrics from before, along with the psychoacoustic Loudness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72499085",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Batch processing a bunch of recordings.\n",
    "\n",
    "The final step is to run all of these metrics on a whole bunch of wav recordings all at once. In the Soundscape Indices (SSID) project for which this package was developed, we use this data to train machine learning models, so it's necessary to process a large number of recordings. Using the predefined analysis settings, we can loop through an entire folder, add it all to the results dataframe and come away with a single spreadsheet of all of the metrics needed.\n",
    "\n",
    "To start, we'll demonstrate this using a normal `for` loop. The `for` loop will process one recording at a time and add it to our results dataframe. For the demonstration, we'll reload all of the variables needed and use the default analysis settings to run all of the metrics and time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "607fb96d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T14:47:44.271476Z",
     "start_time": "2023-08-09T14:36:35.370997Z"
    }
   },
   "source": [
    "import time # Just for timing\n",
    "from tqdm import tqdm\n",
    "\n",
    "wav_folder = Path().cwd().parent.parent.joinpath(\"test\", \"data\")\n",
    "levels = wav_folder.joinpath(\"Levels.json\")\n",
    "with open(levels) as f:\n",
    "    levels = json.load(f)\n",
    "df = prep_multiindex_df(levels, incl_metric=False)\n",
    "\n",
    "analysis_settings = AnalysisSettings.default()\n",
    "analysis_settings['scikit-maad']['all_temporal_alpha_indices']['run'] = True\n",
    "analysis_settings['scikit-maad']['all_spectral_alpha_indices']['run'] = True\n",
    "# analysis_settings[\"MoSQITo\"][\"sharpness_din_from_loudness\"][\"run\"] = False\n",
    "# analysis_settings[\"MoSQITo\"][\"sharpness_din_perseg\"][\"run\"] = False\n",
    "# analysis_settings[\"MoSQITo\"][\"roughness_dw\"][\"run\"] = False\n",
    "\n",
    "begin = time.perf_counter() # Start timer\n",
    "\n",
    "# Loop through each wav file in the folder\n",
    "for wav in (pbar := tqdm(list(wav_folder.glob(\"*.wav\")))):\n",
    "    recording = wav.stem\n",
    "    pbar.set_description(f\"Processing {recording}\")\n",
    "    decibel = tuple(levels[recording].values())\n",
    "    b = Binaural.from_wav(wav, calibrate_to=decibel)\n",
    "    df = add_results(df, b.process_all_metrics(analysis_settings, verbose=False, parallel=True)) # Process all metrics and add to results df\n",
    "\n",
    "end = time.perf_counter() # Stop timer"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91845ce0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T14:47:56.228053Z",
     "start_time": "2023-08-09T14:47:56.207172Z"
    }
   },
   "source": [
    "print(\"Run time on 16 core AMD Ryzen 7 4800HS (Windows 11): 1781.15 seconds (29.69 minutes)\")\n",
    "\n",
    "print(\"Run on M2 Max:\")\n",
    "print(f\"Time taken (using for loop): {end-begin: .2f} seconds ({(end-begin)/60: .2f} minutes)\")\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eaf63976",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "As we can see, this took quite a long time. The problem is that the MoSQITo metrics can take up to a minute for each channel of each recording and the for loop only processes one recording at a time. Since most modern computers are multicore, this leaves a ton of processing power unused. On my machine, I have 16 cores and at any one time during this process, only two of those cores are running at 100%.\n",
    "\n",
    "In order to take full advantage of the other cores, we need to tell the computer to process multiple recordings at once and add them all together later. This is called parallel processing and could theoretically speed up the analysis by 8x (usually we don't get this full speed up though). The reason two cores are running in parallel above is because we're already running both channels at the same time for the MoSQITo metrics.\n",
    "\n",
    "To do this, we provide a function called `parallel_process()` which takes the path to your wav folder as an argument, then performs our full processing on multiple recordings in parallel and returns the full result dataframe at the end.\n",
    "\n",
    "Note: Don't worry about the status updates looking  a bit jumbled - that's what happens with parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f674d1fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:35:16.638522Z",
     "start_time": "2023-08-12T20:32:10.289526Z"
    }
   },
   "source": [
    "import time\n",
    "from soundscapy.analysis.parallel_processing import parallel_process\n",
    "\n",
    "# Redefine path etc. just for the example\n",
    "wav_folder = Path().cwd().parent.parent.joinpath(\"test\", \"data\")\n",
    "levels = wav_folder.joinpath(\"Levels.json\")\n",
    "with open(levels) as f:\n",
    "    levels = json.load(f)\n",
    "df = prep_multiindex_df(levels, incl_metric=False)\n",
    "\n",
    "analysis_settings = AnalysisSettings.default()\n",
    "analysis_settings['scikit-maad']['all_temporal_alpha_indices']['run'] = True\n",
    "analysis_settings['scikit-maad']['all_spectral_alpha_indices']['run'] = True\n",
    "# analysis_settings[\"MoSQITo\"][\"sharpness_din_from_loudness\"][\"run\"] = False\n",
    "# analysis_settings[\"MoSQITo\"][\"sharpness_din_perseg\"][\"run\"] = False\n",
    "# analysis_settings[\"MoSQITo\"][\"roughness_dw\"][\"run\"] = False\n",
    "\n",
    "start = time.perf_counter() # Start timer\n",
    "\n",
    "df = parallel_process(\n",
    "    wav_folder.glob(\"*.wav\"), df, levels, analysis_settings, verbose=False\n",
    ")\n",
    "\n",
    "stop = time.perf_counter()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "040b8d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:35:16.650071Z",
     "start_time": "2023-08-12T20:35:16.639894Z"
    }
   },
   "source": [
    "print(\"Previous run time (16 core AMD Ryzen 7 4800HS, Windows 11): 681.61 seconds (11.36 minutes)\")\n",
    "\n",
    "print(\"Run on M2 Max:\")\n",
    "print(f\"Time taken: {stop-start:.2f} seconds ({(stop-start)/60: .2f} minutes)\")\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "08293ca5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "That's three times as fast! \n",
    "\n",
    "Then save it if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5001a1b",
   "metadata": {},
   "source": [
    "# from datetime import datetime\n",
    "# df.to_excel(wav_folder.joinpath(\"test\", f\"ParallelTest_{datetime.today().strftime('%Y-%m-%d')}.xlsx\"))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc136cb",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "soundscapy-dev-3.7-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "839cba8ae7f7082c5bcb3c590fd12f2b3a15a875e42d2935e5155aa711605eac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

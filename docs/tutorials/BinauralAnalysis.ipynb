{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b060e6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Using Soundscapy for Binaural Recording Analysis\n",
    "\n",
    "In v0.3.2, the ability to perform a huge suite of (psycho)acoustic analyses has been added to Soundscapy. This has been optimised for performing batch processing of many recordings, ease of use, and reproducibility. To do this, we rely on three packages to provide the analysis functions:\n",
    "\n",
    " * [Python Acoustics](https://github.com/python-acoustics/python-acoustics) (`acoustics`) : Python Acoustics is a library aimed at acousticians. It provides two huge benefits - first, all of the analysis functions are referenced directly to the relevant standard. Second, we subclass their `Signal` class to provide the Binaural functionality and any function available within the `Signal` class is also available to Soundscapy's `Binaural` class.\n",
    " * [scikit-maad](https://scikit-maad.github.io) (`maad`) : scikit-maad is a modular toolbox for quantitiative soundscape analysis, focussed on ecological soundscapes and bioacoustic indices. scikit-maad provides a huge suite of ecosoundscape focussed indices, including Acoustic Richness Index, Acoustic Complexity Index, Normalized Difference Soundscape Index (NDSI), and more.\n",
    " * [MoSQITo](https://github.com/Eomys/MoSQITo) (`mosqito`): MoSQITo is a modular framework of key sound quality metrics, providing the psychoacoustic metrics.\n",
    "\n",
    "The metrics currently available are:\n",
    "* Python Acoustics : $L_{Zeq}$, $L_{Aeq}$, $L_{Ceq}$, SEL, and all associated statistics ($L_5$ through $L_{95}$, $L_{max}$ and $L_{min}$, as well as [kurtosis](https://acousticstoday.org/wp-content/uploads/2020/12/Kurtosis-A-New-Tool-for-Noise-Analysis-Wei-Qiu-William-J.-Murphy-and-Alice-Suter.pdf) and skewness.\n",
    "* scikit-maad : So far we have only implemented the combined `all_temporal_alpha_indices` and `all_spectral_alpha_indices` from `scikit-maad`; calculating them individually is not yet supported. `all_temporal_alpha_indices` comprises 16 temporal domain acoustic indices, such as temporal signal-to-noise-ratio, temporal entropy, temporal events. `all_spectral_alpha_indices` comprises 19 spectral domain acoustic indices, such as Bioacoustic Index, Acoustic Diversity Index, NDSI, Acoustic Evenness Index.\n",
    "* MoSQITo :\n",
    "    * Loudness (Zwicker time varying),\n",
    "    * Sharpness (time varying, from loudness, and per segment with DIN, Aures, Bismarck, and Fastl weightings),\n",
    "    * Roughness (Daniel and Weber, 1997).\n",
    "\n",
    "Soundscapy combines all of these metrics and makes it easy and (relatively) fast to compute any or all of them for a binaural audio recording. These results have been preliminarily confirmed through comparison of results obtained from Head Acoustics ArtemiS suite on a set of real-world recordings.\n",
    "\n",
    "## Consistent Analysis Settings\n",
    "\n",
    "A primary goal when developing this library was to make it easy to save and document the settings used for all analyses. This is done through a `settings.yaml` file and the `AnalysisSettings` class. Although the settings for each metric can be set at runtime, the `settings.yaml` file allows you to set all of the settings at once and document exactly what settings were passed to each analysis function and to share these settings with collaborators or reviewers.\n",
    "\n",
    "## Batch processing\n",
    "The other primary goal was to make it simple and fast to perform this analysis on many recordings. One aspect of this is unifying the outputs from the underlying libraries and presenting them in an easy to parse format. The analysis functions from Soundscapy can return a MultiIndex pandas DataFrame with the Recording filename and Left and Right channels in the index and a column for each metric calculated. This dataframe can then be easily saved to a .csv or Excel spreadsheet. Alternatively, a dictionary can be returned for further processing within Python. The key point is that after calculating 100+ metrics for 1,000+ recordings, you'll be left with a single tidy spreadsheet.\n",
    "\n",
    "The Soundscape Indices (SSID) project for which this was developed has over 3,000 recordings for which we needed to calculate a full suite of metrics for both channels. In particular, the MoSQITo functions can be quite slow, so running each recording one at a time can be prohibitively slow and only utilize a small portion of the available computing power. To help with this, a set of simple functions is provided to enable parallel processing, such that multiple recordings can be processed simultaneously by a multi-core CPU. In our initial tests on a 16-core AMD Ryzen 7 4800HS CPU (Fedora Linux 36), this increased the speed for processing 20 recordings by at least 8 times.\n",
    "\n",
    "In testing, the MoSQITo functions are particularly slow, taking up to 3 minutes to calculate the Loudness for a 30s two-channel recording. When running only a single recording through, this has also been sped up by parallelizing the per-channel calculation, reducing the computation time to around 50s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91ed01",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "The basis of all of our analysis starts with the binaural signal, so we begin by importing the `Binaural` class. We'll also load up and examine our analysis settings. Throughout Soundscapy, we use `pathlib.Path` for defining filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf0f9fb",
   "metadata": {
    "lines_to_next_cell": 2,
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:31.913447Z",
     "start_time": "2023-08-12T20:19:30.930268Z"
    }
   },
   "source": [
    "# Add soundscapy to the Python path\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "# imports\n",
    "from soundscapy import Binaural\n",
    "from soundscapy import AnalysisSettings\n",
    "from soundscapy.analysis.binaural import prep_multiindex_df, add_results, process_all_metrics\n",
    "import json\n",
    "from pathlib import Path"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "68fd3ee4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Set up where the data is located. In this case, we'll use the sample recordings located under the `test` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f299664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:31.919300Z",
     "start_time": "2023-08-12T20:19:31.914511Z"
    }
   },
   "source": [
    "# May need to adjust for your system\n",
    "wav_folder = Path().cwd().parent.parent.joinpath(\"test\", \"data\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "556fc578",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Ensuring that Soundscapy knows exactly how loud your recordings were onsite is crucial to getting correct answers. If you used equipment such as the Head Acoustics SqoBold, and were careful about how the recordings are exported to .wav, then they may already be correctly adjusted (as ours are here). However its best to be safe and calibrate each signal to their real-world dB level. To do this, we load in a .json that contains the per-channel correct dB $L_{eq}$ level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f78f14c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:31.920661Z",
     "start_time": "2023-08-12T20:19:31.916705Z"
    }
   },
   "source": [
    "levels = wav_folder.joinpath(\"Levels.json\")\n",
    "\n",
    "with open(levels) as f:\n",
    "    levels = json.load(f)\n",
    "\n",
    "# Look at the first five sets of levels\n",
    "list(levels.items())[:5]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "34481ae7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "### Prepping the results dataframe\n",
    "The easiest way to organise and add the new data as it is processed is to prepare a dataframe ahead of time. We've provided a small function to convert a dictionary of calibration levels (`level`) into the properly formatted dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb2fc0c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:31.981839Z",
     "start_time": "2023-08-12T20:19:31.968684Z"
    }
   },
   "source": [
    "df = prep_multiindex_df(levels, incl_metric=True)\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8e138497",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "#### Load in a Binaural recording\n",
    "Load in a binaural wav signal. We can use the `plot` function provided by the `acoustics.Signal` super-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76bc2e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:34.148763Z",
     "start_time": "2023-08-12T20:19:32.948873Z"
    }
   },
   "source": [
    "binaural_wav = wav_folder.joinpath(\"CT101.wav\")\n",
    "b = Binaural.from_wav(binaural_wav)\n",
    "b.plot();"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a17c3655",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "To ensure that the dB level is correct, and therefore any other metrics are correct, we start by calibrating the signal to precalculated levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68dc1464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:34.214107Z",
     "start_time": "2023-08-12T20:19:34.149622Z"
    }
   },
   "source": [
    "decibel = (levels[b.recording][\"Left\"], levels[b.recording][\"Right\"])\n",
    "print(f\"Calibration levels: {decibel}\")\n",
    "b.calibrate_to(decibel, inplace=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "87001293",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now, check it by comparing it to what we already knew were the correct levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b3cedc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:34.841514Z",
     "start_time": "2023-08-12T20:19:34.826178Z"
    }
   },
   "source": [
    "print(f\"Predefined levels: {levels[b.recording]}\")\n",
    "print(f\"Calculated Levels: {b.pyacoustics_metric('Leq', statistics=['avg'], as_df=False)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "407924ea",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Calculating Acoustic Metrics\n",
    "\n",
    "This brings us to how to calculate any of the many metrics available. Let's start simple with $L_{Aeq}$.\n",
    "\n",
    "### Python Acoustics\n",
    "\n",
    "Since the $L_{Aeq}$ calc is provided by the Python Acoustics library, we'll be calling `pyacoustic_metric`. Then, we need to tell it what particular metric we want, what stats to calculate as well, what to label it, and what format to return the results in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35968c0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:35.772475Z",
     "start_time": "2023-08-12T20:19:35.688843Z"
    }
   },
   "source": [
    "metric = \"LAeq\"\n",
    "stats = (\"avg\", 10, 50, 90, 95, \"max\")\n",
    "label = \"LAeq\"\n",
    "b.pyacoustics_metric(metric, stats, label, as_df=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c2969062",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "If we want, we can get the results back as a pandas DataFrame instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1bb36e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:36.991090Z",
     "start_time": "2023-08-12T20:19:36.920368Z"
    }
   },
   "source": [
    "b.pyacoustics_metric(metric, stats, label, as_df=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "be992443",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "And we can easily do the same for the C-weighting level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55da2d6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:19:38.137315Z",
     "start_time": "2023-08-12T20:19:38.055350Z"
    }
   },
   "source": [
    "b.pyacoustics_metric(\"LCeq\", stats, as_df=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f5786a3c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### MoSQITo\n",
    "\n",
    "MoSQITo is very exciting as it is one of the first completely free and open-source libraries for calculating psychoacoustic features. Let's try out calculating the psychoacoustic loudness.\n",
    "\n",
    "We start by defining many of the same options, but with two new ones. The first is our `func_args` to pass to `MoSQITo`. Since our test recording was collected in a public park, we need to select the correct field type: free or diffuse, and pass that to MoSQITo.\n",
    "\n",
    "The second new argument is `parallel`. This just tells Soundscapy whether to try to calculate the Left and Right channels simultaneously to speed up processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce383d8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:09.848885Z",
     "start_time": "2023-08-12T20:19:39.106261Z"
    }
   },
   "source": [
    "metric = \"loudness_zwtv\"\n",
    "stats = (5, 50, 'avg', 'max')\n",
    "func_args = {\n",
    "    'field_type': 'free'\n",
    "}\n",
    "\n",
    "b.mosqito_metric(metric, statistics=stats, as_df=True, parallel=True, verbose=True, func_args=func_args)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fd19d5f1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "`sharpness_din_from_loudness` is a bit of a special case to keep in mind. It can drastically speed up the processing time since it calculates the Sharpness values from pre-calculated Loudness results. If you are planning to do both analyses, I highly suggest using it. Soundscapy will handle it behind the scenes to make sure it doesn't accidentally calculate the Loudness values twice if you've asked for both of them. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6edbbf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:40.423258Z",
     "start_time": "2023-08-12T20:20:09.850003Z"
    }
   },
   "source": [
    "b.mosqito_metric(\"sharpness_din_from_loudness\", stats, as_df=True, parallel=True, verbose=True, func_args=func_args)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "431afc2f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "By default, the metrics will be calculated for both channels. But you may want only a single channel. This can be set with the `channel` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3342978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:40.450543Z",
     "start_time": "2023-08-12T20:20:40.424170Z"
    }
   },
   "source": [
    "b.pyacoustics_metric(\"LZeq\", channel=\"Left\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5e580b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:41.243999Z",
     "start_time": "2023-08-12T20:20:40.444995Z"
    }
   },
   "source": [
    "b.maad_metric(\"all_spectral_alpha_indices\", verbose=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "b.maad_metric(\"all_temporal_alpha_indices\", verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T20:59:10.379510Z",
     "start_time": "2023-08-12T20:59:10.296558Z"
    }
   },
   "id": "ab9bbfd3d017d317",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7ee05cbb",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Defining Analysis Settings\n",
    "\n",
    "Soundscapy provides the ability to predefine your analysis settings. These are defined in a separate `.yaml` file and are managed by Soundscapy using the `AnalysisSettings` class. These settings can then be passed to any of the analysis functions, rather than separately defining your settings as we did above. This will be particularly useful when performing our batch processing on an entire folder of wav recordings later.\n",
    "\n",
    "Soundscapy provides a set of default settings which can be easily loaded in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43f4791c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:41.251435Z",
     "start_time": "2023-08-12T20:20:41.244569Z"
    }
   },
   "source": [
    "analysis_settings = AnalysisSettings.default()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "01cdf86b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "However, in your own analysis you'll probably want to define your own options and load that in. We'll show how this is done using the `example_settings.yaml' file. First, let's take a look at how it's laid out:\n",
    "\n",
    "```\n",
    "# Settings file for batch acoustic analysis.\n",
    "# Split up according to which library performs which analysis.\n",
    "\n",
    "# Python Acoustics\n",
    "# Supported metrics: LAeq, LZeq, LCeq, SEL\n",
    "# Supported stats: avg/mean, max, min, kurt, skew, any integer from 1-99\n",
    "PythonAcoustics:\n",
    "    LAeq:\n",
    "        run: true\n",
    "        main: 'avg'\n",
    "        statistics: [5, 10, 50, 90, 95, 'min', 'max', 'kurt', 'skew']\n",
    "        channel: [\"Left\", \"Right\"]\n",
    "        label: 'LAeq'\n",
    "        func_args:\n",
    "            time: 0.125\n",
    "            method: \"average\"\n",
    "\n",
    "    LZeq:\n",
    "        run: true\n",
    "        main: 'avg'\n",
    "        statistics: [5, 10, 50, 90, 95, 'min', 'max', 'kurt', 'skew']\n",
    "        channel: [\"Left\", \"Right\"]\n",
    "        label: 'LZeq'\n",
    "        func_args:\n",
    "            time: 0.125\n",
    "            method: \"average\"\n",
    "\n",
    "# MoSQITo\n",
    "# supported metrics: loudness_zwtv, sharpness_din_from_loudness, roughness_dw\n",
    "# supported stats: avg/mean, max, min, kurt, skew, any integer from 1-99\n",
    "MoSQITo:\n",
    "    loudness_zwtv:\n",
    "        run: true\n",
    "        main: 5\n",
    "        statistics: [10, 50, 90, 95, 'min', 'max', 'kurt', 'skew', 'avg']\n",
    "        channel: [\"Left\", \"Right\"]\n",
    "        label: \"N\"\n",
    "        parallel: true\n",
    "        func_args:\n",
    "            field_type: \"free\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d69b62",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "The settings file is broken up according to the three libraries. Within these, we define separate options for each metric to calculate. The name of this metric should correspond exactly with what Soundscapy expects (in the case of PythonAcoustics) or what the underlying library calls its function.\n",
    "\n",
    "Within each function, we then have a collection of settings that Soundscapy uses:\n",
    "    * `run` : This tells Soundscapy whether or not to actually run this metric. This allows you to define and save the options you use for each metric without needing to run it.\n",
    "    * `main` and `statistics` : These are the sub-statistics to calculate (e.g. $L_{5}$, $L_{90}$, etc.). `main` operates just like any of these, except you also have the option to return only the main statistic to simplify your results while still leaving your other preferences intact.\n",
    "    * `label` : What label to assign that metric. For instance, Loudness is typically `'N'`. When calculated, the statistics will be appended like so: N_5, N_10, N_avg, ... N_{`stat`} and this will be the column name for that metric. If you pass nothing here, then Soundscapy will fall back to the labels defined in `sq_metrics.DEFAULT_LABELS`. **Warning**: Some functions share a label (e.g. `sharpness_din_tv` and `sharpness_din_perseg` are both 'S'), if you run both of these and don't define different labels, one will overwrite the other.\n",
    "\n",
    "Finally, there is an opportunity to define arguments to pass to the underlying function itself. This is perhaps the most important part for consistency and reproducibility. This is where you define which standard is being used, what time or frequency weighting, or what spectrogram bins to use. These options are defined by the 3 analysis libraries used and are not documented fully in Soundscapy. When the `AnalysisSettings` is parsed, `func_args` will be returned as a `dict` with an entry for each option you'd like to pass. `func_args` is then passed as `**kwargs` to e.g. the `mosqito.sq_metrics.loudness_zwtv()` function. If `func_args` contains an option the function doesn't recognise it will throw an error, so be careful when defining these arguments.\n",
    "\n",
    "---------------------------------------\n",
    "Let's try loading in the `example_settings.yaml` file and see how `AnalysisSettings` handles it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "424399f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:44.485117Z",
     "start_time": "2023-08-12T20:20:44.481932Z"
    }
   },
   "source": [
    "ex_settings = AnalysisSettings.from_yaml(Path(\"example_settings.yaml\"))\n",
    "ex_settings['scikit-maad']['all_temporal_alpha_indices']['run'] = True\n",
    "ex_settings['scikit-maad']['all_spectral_alpha_indices']['run'] = True\n",
    "ex_settings"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4ddfdb81",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "`ex_settings` is just a Python `dict` with some class methods added on. One of these is a function to parse the settings object for a specific library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6c6f7fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:45.718584Z",
     "start_time": "2023-08-12T20:20:45.705121Z"
    }
   },
   "source": [
    "ex_settings.parse_pyacoustics(metric=\"LAeq\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e822f46b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "This returns the value for `run`, `channel`, `statistics`, `label`, and `func_args` which will then be used by the `pyacoustic_metric()` function to calculate the $L_{Aeq}$ and its stats.\n",
    "\n",
    "When passing your settings to an analysis function, it will start by automatically parsing and applying the settings for that particular metric. This will override any other settings passed to the function, so if you're using a settings file and you want to change anything, you should either change it in the `.yaml` and reload the settings. This also makes sure you keep a record of the settings for the last time you ran the analysis.\n",
    "\n",
    "You can easily reload the settings `.yaml` after changing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5da5ee5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:47.717749Z",
     "start_time": "2023-08-12T20:20:47.704543Z"
    }
   },
   "source": [
    "# ex_settings = ex_settings.reload()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "89382227",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Running a single metric with predefined settings\n",
    "\n",
    "Now, with our settings loaded, we process a recording using those settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2700bdee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:50.182201Z",
     "start_time": "2023-08-12T20:20:50.095370Z"
    }
   },
   "source": [
    "b.pyacoustics_metric(\"LAeq\", analysis_settings=ex_settings)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0314af2c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "But this is just the start of what makes the analysis settings so useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a584619",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Processing all the metrics at once, using predefined analysis settings.\n",
    "\n",
    "Since we can define the settings for all the metrics, and specify which metrics we want to run, we can process all of our desired metrics at once. `process_all_metrics()` let's us do this with just a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e6ebf83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:20:53.694900Z",
     "start_time": "2023-08-12T20:20:52.711936Z"
    }
   },
   "source": [
    "b.process_all_metrics(ex_settings, verbose=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "61fde748",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "In this case we left out the MoSQITo metrics since they take a while, but let's say you do want to run those. You could either edit the `example_settings.yaml` file and use `ex_settings.reload()`, or what we'll do within this notebook is just directly edit the underlying dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dc4cbd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:21:27.391295Z",
     "start_time": "2023-08-12T20:20:55.665253Z"
    }
   },
   "source": [
    "ex_settings['MoSQITo'][\"loudness_zwtv\"][\"run\"] = True\n",
    "\n",
    "b.process_all_metrics(ex_settings, verbose=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1d20f438",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "And now we have all of the same metrics from before, along with the psychoacoustic Loudness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72499085",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Batch processing a bunch of recordings.\n",
    "\n",
    "The final step is to run all of these metrics on a whole bunch of wav recordings all at once. In the Soundscape Indices (SSID) project for which this package was developed, we use this data to train machine learning models, so it's necessary to process a large number of recordings. Using the predefined analysis settings, we can loop through an entire folder, add it all to the results dataframe and come away with a single spreadsheet of all of the metrics needed.\n",
    "\n",
    "To start, we'll demonstrate this using a normal `for` loop. The `for` loop will process one recording at a time and add it to our results dataframe. For the demonstration, we'll reload all of the variables needed and use the default analysis settings to run all of the metrics and time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "607fb96d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T14:47:44.271476Z",
     "start_time": "2023-08-09T14:36:35.370997Z"
    }
   },
   "source": [
    "import time # Just for timing\n",
    "from tqdm import tqdm\n",
    "\n",
    "wav_folder = Path().cwd().parent.parent.joinpath(\"test\", \"data\")\n",
    "levels = wav_folder.joinpath(\"Levels.json\")\n",
    "with open(levels) as f:\n",
    "    levels = json.load(f)\n",
    "df = prep_multiindex_df(levels, incl_metric=False)\n",
    "\n",
    "analysis_settings = AnalysisSettings.default()\n",
    "analysis_settings['scikit-maad']['all_temporal_alpha_indices']['run'] = True\n",
    "analysis_settings['scikit-maad']['all_spectral_alpha_indices']['run'] = True\n",
    "# analysis_settings[\"MoSQITo\"][\"sharpness_din_from_loudness\"][\"run\"] = False\n",
    "# analysis_settings[\"MoSQITo\"][\"sharpness_din_perseg\"][\"run\"] = False\n",
    "# analysis_settings[\"MoSQITo\"][\"roughness_dw\"][\"run\"] = False\n",
    "\n",
    "begin = time.perf_counter() # Start timer\n",
    "\n",
    "# Loop through each wav file in the folder\n",
    "for wav in (pbar := tqdm(list(wav_folder.glob(\"*.wav\")))):\n",
    "    recording = wav.stem\n",
    "    pbar.set_description(f\"Processing {recording}\")\n",
    "    decibel = tuple(levels[recording].values())\n",
    "    b = Binaural.from_wav(wav, calibrate_to=decibel)\n",
    "    df = add_results(df, b.process_all_metrics(analysis_settings, verbose=False, parallel=True)) # Process all metrics and add to results df\n",
    "\n",
    "end = time.perf_counter() # Stop timer"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91845ce0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T14:47:56.228053Z",
     "start_time": "2023-08-09T14:47:56.207172Z"
    }
   },
   "source": [
    "print(\"Run time on 16 core AMD Ryzen 7 4800HS (Windows 11): 1781.15 seconds (29.69 minutes)\")\n",
    "\n",
    "print(\"Run on M2 Max:\")\n",
    "print(f\"Time taken (using for loop): {end-begin: .2f} seconds ({(end-begin)/60: .2f} minutes)\")\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eaf63976",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "As we can see, this took quite a long time. The problem is that the MoSQITo metrics can take up to a minute for each channel of each recording and the for loop only processes one recording at a time. Since most modern computers are multicore, this leaves a ton of processing power unused. On my machine, I have 16 cores and at any one time during this process, only two of those cores are running at 100%.\n",
    "\n",
    "In order to take full advantage of the other cores, we need to tell the computer to process multiple recordings at once and add them all together later. This is called parallel processing and could theoretically speed up the analysis by 8x (usually we don't get this full speed up though). The reason two cores are running in parallel above is because we're already running both channels at the same time for the MoSQITo metrics.\n",
    "\n",
    "To do this, we provide a function called `parallel_process()` which takes the path to your wav folder as an argument, then performs our full processing on multiple recordings in parallel and returns the full result dataframe at the end.\n",
    "\n",
    "Note: Don't worry about the status updates looking  a bit jumbled - that's what happens with parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f674d1fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:35:16.638522Z",
     "start_time": "2023-08-12T20:32:10.289526Z"
    }
   },
   "source": [
    "import time\n",
    "from soundscapy.analysis.parallel_processing import parallel_process\n",
    "\n",
    "# Redefine path etc. just for the example\n",
    "wav_folder = Path().cwd().parent.parent.joinpath(\"test\", \"data\")\n",
    "levels = wav_folder.joinpath(\"Levels.json\")\n",
    "with open(levels) as f:\n",
    "    levels = json.load(f)\n",
    "df = prep_multiindex_df(levels, incl_metric=False)\n",
    "\n",
    "analysis_settings = AnalysisSettings.default()\n",
    "analysis_settings['scikit-maad']['all_temporal_alpha_indices']['run'] = True\n",
    "analysis_settings['scikit-maad']['all_spectral_alpha_indices']['run'] = True\n",
    "# analysis_settings[\"MoSQITo\"][\"sharpness_din_from_loudness\"][\"run\"] = False\n",
    "# analysis_settings[\"MoSQITo\"][\"sharpness_din_perseg\"][\"run\"] = False\n",
    "# analysis_settings[\"MoSQITo\"][\"roughness_dw\"][\"run\"] = False\n",
    "\n",
    "start = time.perf_counter() # Start timer\n",
    "\n",
    "df = parallel_process(\n",
    "    wav_folder.glob(\"*.wav\"), df, levels, analysis_settings, verbose=False\n",
    ")\n",
    "\n",
    "stop = time.perf_counter()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "040b8d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T20:35:16.650071Z",
     "start_time": "2023-08-12T20:35:16.639894Z"
    }
   },
   "source": [
    "print(\"Previous run time (16 core AMD Ryzen 7 4800HS, Windows 11): 681.61 seconds (11.36 minutes)\")\n",
    "\n",
    "print(\"Run on M2 Max:\")\n",
    "print(f\"Time taken: {stop-start:.2f} seconds ({(stop-start)/60: .2f} minutes)\")\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "08293ca5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "That's three times as fast! \n",
    "\n",
    "Then save it if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5001a1b",
   "metadata": {},
   "source": [
    "# from datetime import datetime\n",
    "# df.to_excel(wav_folder.joinpath(\"test\", f\"ParallelTest_{datetime.today().strftime('%Y-%m-%d')}.xlsx\"))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc136cb",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "soundscapy-dev-3.7-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "839cba8ae7f7082c5bcb3c590fd12f2b3a15a875e42d2935e5155aa711605eac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
